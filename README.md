# ğŸ­ Emotion Recognition System
### Deep Learning-Based Facial Emotion Classification

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://modul6ml-uaphaidar-dimas-heryanto.streamlit.app/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.15+](https://img.shields.io/badge/TensorFlow-2.15+-orange.svg)](https://www.tensorflow.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

<div align="center">
  <img src="https://img.shields.io/badge/Accuracy-87%25-brightgreen" alt="Best Accuracy">
  <img src="https://img.shields.io/badge/Models-4-blue" alt="Models">
  <img src="https://img.shields.io/badge/Classes-3-purple" alt="Classes">
</div>

---

## ğŸ“‹ Table of Contents
- [Overview](#-overview)
- [Dataset](#-dataset)
- [Models Architecture](#-models-architecture)
- [Performance Comparison](#-performance-comparison)
- [Key Features](#-key-features)
- [Installation](#-installation)
- [Usage](#-usage)
- [Results & Analysis](#-results--analysis)
- [Live Demo](#-live-demo)
- [Project Structure](#-project-structure)
- [Technologies Used](#-technologies-used)
- [Contributors](#-contributors)

---

## ğŸ¯ Overview

This project implements a **comprehensive emotion recognition system** using deep learning techniques to classify facial expressions into three primary emotions: **Angry**, **Sad**, and **Surprise**. The system compares four different neural network architectures to identify the most effective model for emotion detection.

### ğŸ“ Academic Context
- **Course**: Machine Learning - Semester 7
- **Institution**: [Your University Name]
- **Project Type**: Final Assignment (UAP)

### ğŸŒŸ Project Highlights
- âœ… Four state-of-the-art deep learning models
- âœ… Comprehensive error analysis and visualization
- âœ… Interactive web-based interface
- âœ… Batch and single image prediction
- âœ… Real-time emotion detection
- âœ… Detailed performance metrics

---

## ğŸ“Š Dataset

### Dataset Information
- **Source**: [Kaggle - Emotion Recognition Dataset](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset)
- **Total Images**: 6,470 images
- **Classes**: 3 emotions (Angry, Sad, Surprise)
- **Image Format**: Grayscale/RGB facial images
- **Image Size**: Variable (resized to 128x128 for training)

### Class Distribution

| Emotion | Training Samples | Validation Samples | Total |
|---------|-----------------|-------------------|--------|
| **Angry** | 210 | 52 | 262 |
| **Sad** | 629 | 157 | 786 |
| **Surprise** | 197 | 49 | 246 |
| **Total** | 1,036 | 258 | **1,294** |

### ğŸ“ˆ Data Distribution Visualization
```
Sad (60.7%)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Angry (20.2%)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Surprise (19.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

### âš–ï¸ Class Imbalance Handling
To address the class imbalance, we implemented:
- **Class Weights**: Computed weights inversely proportional to class frequencies
  - Angry: 1.65
  - Sad: 0.55
  - Surprise: 1.75
- **Data Augmentation**: Enhanced minority class representation

---

## ğŸ—ï¸ Models Architecture

### 1ï¸âƒ£ Custom CNN (Convolutional Neural Network)

**Architecture Overview:**
```
Input (128x128x3)
    â†“
Conv2D(32) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â†“
Conv2D(64) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â†“
Conv2D(128) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â†“
Conv2D(256) â†’ BatchNorm â†’ GlobalAvgPool
    â†“
Dense(256) â†’ BatchNorm â†’ Dropout(0.5)
    â†“
Dense(128) â†’ Dropout(0.3)
    â†“
Dense(3, softmax)
```

**Key Features:**
- 4 Convolutional blocks with increasing filters (32 â†’ 64 â†’ 128 â†’ 256)
- Batch Normalization after each conv layer
- L2 Regularization (0.001)
- Global Average Pooling to reduce overfitting
- Total Parameters: ~3.5M

**Training Configuration:**
- Optimizer: Adam (lr=0.001)
- Loss: Categorical Crossentropy
- Epochs: 30 (with EarlyStopping)
- Batch Size: 32

---

### 2ï¸âƒ£ ResNet50 (Residual Network)

**Architecture Overview:**
```
ResNet50 (ImageNet Pretrained)
    â†“
GlobalAveragePooling2D
    â†“
BatchNormalization
    â†“
Dense(512, relu, L2) â†’ Dropout(0.5) â†’ BatchNorm
    â†“
Dense(256, relu, L2) â†’ Dropout(0.4) â†’ BatchNorm
    â†“
Dense(128, relu, L2) â†’ Dropout(0.3)
    â†“
Dense(3, softmax)
```

**Transfer Learning Strategy:**
- **Stage 1**: Train custom head (frozen base) - 15 epochs, LR: 1e-4
- **Stage 2**: Fine-tune top 30 layers - 25 epochs, LR: 1e-5
- Total Parameters: ~25.6M
- Trainable Parameters: ~8.2M

**Preprocessing:**
- Uses ResNet50-specific preprocessing (`preprocess_input`)
- Mean subtraction and scaling based on ImageNet

---

### 3ï¸âƒ£ MobileNetV2 (Efficient Mobile Architecture)

**Architecture Overview:**
```
MobileNetV2 (ImageNet Pretrained)
    â†“
GlobalAveragePooling2D
    â†“
BatchNormalization
    â†“
Dense(512, relu, L2) â†’ Dropout(0.5) â†’ BatchNorm
    â†“
Dense(256, relu, L2) â†’ Dropout(0.4) â†’ BatchNorm
    â†“
Dense(128, relu, L2) â†’ Dropout(0.3)
    â†“
Dense(3, softmax)
```

**Transfer Learning Strategy:**
- **Stage 1**: Train custom head (frozen base) - 15 epochs, LR: 1e-4
- **Stage 2**: Fine-tune top 30 layers - 25 epochs, LR: 1e-5
- Total Parameters: ~3.5M
- Trainable Parameters: ~1.8M

**Key Advantages:**
- Lightweight and efficient
- Suitable for mobile/edge deployment
- Faster inference time compared to ResNet50

**Preprocessing:**
- Uses MobileNetV2-specific preprocessing
- Scales inputs to [-1, 1] range

---

### 4ï¸âƒ£ VGG16 (Visual Geometry Group)

**Architecture Overview:**
```
VGG16 (ImageNet Pretrained)
    â†“
GlobalAveragePooling2D
    â†“
BatchNormalization
    â†“
Dense(512, relu, L2) â†’ Dropout(0.5) â†’ BatchNorm
    â†“
Dense(256, relu, L2) â†’ Dropout(0.4) â†’ BatchNorm
    â†“
Dense(128, relu, L2) â†’ Dropout(0.3)
    â†“
Dense(3, softmax)
```

**Three-Stage Training Strategy:**
- **Stage 1**: Train custom head (frozen base) - 20 epochs, LR: 1e-4
- **Stage 2**: Fine-tune Block5 - 25 epochs, LR: 5e-5
- **Stage 3**: Fine-tune Block4+5 - 20 epochs, LR: 1e-5
- Total Parameters: ~16.8M
- Trainable Parameters: ~9.2M

**VGG16 Architecture Details:**
- 5 convolutional blocks (Block1-5)
- Gradual unfreezing for optimal fine-tuning
- Deep architecture with strong feature extraction

**Preprocessing:**
- Uses VGG16-specific preprocessing
- Mean subtraction based on ImageNet statistics

---

## ğŸ“Š Performance Comparison

### ğŸ† Overall Accuracy Comparison

| Model | Accuracy | Precision | Recall | F1-Score | Parameters | Training Time |
|-------|----------|-----------|--------|----------|------------|---------------|
| **VGG16** | **87.0%** | **87%** | **85%** | **84%** | 16.8M | ~65 epochs |
| **CNN** | **79.0%** | **80%** | **76%** | **75%** | 3.5M | ~30 epochs |
| **ResNet50** | **73.0%** | **72%** | **63%** | **65%** | 25.6M | ~40 epochs |
| **MobileNetV2** | **70.0%** | **69%** | **63%** | **63%** | 3.5M | ~40 epochs |

### ğŸ“ˆ Performance Visualization
```
Overall Accuracy
VGG16        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  87%
CNN          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      79%
ResNet50     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         73%
MobileNetV2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              70%
```

---

## ğŸ¯ Detailed Per-Class Performance

### 1. Angry Emotion

| Model | Precision | Recall | F1-Score | Accuracy |
|-------|-----------|--------|----------|----------|
| **VGG16** | **0.77** | **0.82** | **0.80** | **82.4%** |
| **CNN** | **0.60** | **0.73** | **0.66** | **73.3%** |
| **ResNet50** | **0.53** | **0.42** | **0.47** | **42.4%** |
| **MobileNetV2** | **0.51** | **0.45** | **0.48** | **45.4%** |

**Winner: VGG16** âœ…
- Highest recall (82%) - best at detecting angry emotions
- Strong confidence gap (0.115) between correct and incorrect predictions

---

### 2. Sad Emotion

| Model | Precision | Recall | F1-Score | Accuracy |
|-------|-----------|--------|----------|----------|
| **VGG16** | **0.92** | **0.89** | **0.91** | **89.4%** |
| **CNN** | **0.87** | **0.84** | **0.85** | **83.7%** |
| **ResNet50** | **0.76** | **0.88** | **0.82** | **87.5%** |
| **MobileNetV2** | **0.77** | **0.80** | **0.79** | **80.4%** |

**Winner: VGG16** âœ…
- Exceptional precision (92%) and F1-score (91%)
- Most reliable for sad emotion detection
- Highest confidence gap (0.258)

---

### 3. Surprise Emotion

| Model | Precision | Recall | F1-Score | Accuracy |
|-------|-----------|--------|----------|----------|
| **VGG16** | **0.82** | **0.84** | **0.83** | **83.7%** |
| **CNN** | **0.78** | **0.70** | **0.74** | **69.5%** |
| **ResNet50** | **0.79** | **0.59** | **0.67** | **58.5%** |
| **MobileNetV2** | **0.63** | **0.63** | **0.63** | **62.6%** |

**Winner: VGG16** âœ…
- Balanced precision and recall
- Strong performance across all metrics

---

## ğŸ” Error Analysis

### VGG16 (Best Model) - Detailed Analysis

#### Confusion Patterns

| True Label | Predicted | Error Count | Percentage | Avg Confidence |
|------------|-----------|-------------|------------|----------------|
| Sad | Angry | 50 | 60.2% of Sad errors | 0.680 |
| Angry | Sad | 35 | 76.1% of Angry errors | 0.756 |
| Sad | Surprise | 33 | 39.8% of Sad errors | 0.645 |
| Surprise | Sad | 26 | 65.0% of Surprise errors | 0.757 |
| Surprise | Angry | 14 | 35.0% of Surprise errors | 0.652 |
| Angry | Surprise | 11 | 23.9% of Angry errors | 0.661 |

#### Key Insights:
1. **Sad â†” Angry** is the most common confusion (85 total errors)
2. **Surprise â†’ Sad** is the second most common (26 errors)
3. Model shows good confidence discrimination:
   - Correct predictions: **0.905 avg confidence**
   - Incorrect predictions: **0.697 avg confidence**
   - **Confidence Gap: 0.208** (strong reliability indicator)

#### Error Distribution by Confidence Level

| Confidence Level | Error Count | Percentage |
|------------------|-------------|------------|
| Low (<0.6) | 42 | 24.9% |
| Medium (0.6-0.8) | 58 | 34.3% |
| High (â‰¥0.8) | 69 | 40.8% |

âš ï¸ **Important Finding**: 40.8% of errors occur with high confidence, indicating systematic misclassification patterns rather than model uncertainty.

---

### CNN Model - Error Analysis

#### Confusion Matrix Highlights:
- **Angry â†’ Sad**: 63 errors (90% of Angry errors)
- **Sad â†’ Angry**: 87 errors (68% of Sad errors)
- **Surprise â†’ Angry**: 40 errors (53.3% of Surprise errors)

#### Confidence Analysis:
- Correct predictions: **0.871 confidence**
- Incorrect predictions: **0.708 confidence**
- Confidence Gap: **0.163**

**Observation**: Good confidence discrimination, but lower overall accuracy than VGG16.

---

### ResNet50 - Error Analysis

#### Major Issues:
1. **Poor Angry Detection**: Only 42.4% recall
   - 145 Angry images misclassified as Sad (96% of errors)
   - **Negative confidence gap** (-0.005): Model equally confident when wrong!

2. **Sad Classification**: Strong (87.5% recall)
   - Best performing class for this model

3. **Surprise Detection**: Moderate (58.5% recall)
   - 69 misclassified as Sad (67.6% of errors)

**Critical Finding**: ResNet50 shows **class bias** toward Sad emotion.

---

### MobileNetV2 - Error Analysis

#### Confusion Patterns:
- **Angry â†’ Sad**: 114 errors (79.7% of Angry errors)
- **Sad â†’ Angry**: 92 errors (59.7% of Sad errors)
- **Surprise â†’ Sad**: 71 errors (77.2% of Surprise errors)

#### Confidence Analysis:
- Correct: **0.753**
- Incorrect: **0.626**
- Gap: **0.127** (lowest among all models)

**Observation**: Model shows uncertainty, reflected in lower confidence scores.

---

## ğŸ“Š Comprehensive Model Comparison

### Strengths & Weaknesses

| Model | âœ… Strengths | âš ï¸ Weaknesses | ğŸ’¡ Best Use Case |
|-------|-------------|---------------|------------------|
| **VGG16** | â€¢ Highest accuracy (87%)<br>â€¢ Excellent across all classes<br>â€¢ Strong confidence discrimination<br>â€¢ Best angry/surprise detection | â€¢ Largest model size (16.8M)<br>â€¢ Slower inference<br>â€¢ Requires more resources | Production environments where accuracy is critical |
| **CNN** | â€¢ Good balance (79%)<br>â€¢ Fast training<br>â€¢ Smaller model size<br>â€¢ No pretrained dependency | â€¢ Lower accuracy than VGG16<br>â€¢ Moderate angry detection<br>â€¢ More training needed | Resource-constrained environments, embedded systems |
| **ResNet50** | â€¢ Strong sad detection (88%)<br>â€¢ Transfer learning benefits<br>â€¢ Proven architecture | â€¢ Poor angry detection (42%)<br>â€¢ Class bias issues<br>â€¢ Largest parameter count | Scenarios where sad emotion is priority |
| **MobileNetV2** | â€¢ Most efficient (3.5M params)<br>â€¢ Fast inference<br>â€¢ Mobile-ready<br>â€¢ Lightweight | â€¢ Lowest overall accuracy (70%)<br>â€¢ Low confidence scores<br>â€¢ Needs more tuning | Mobile apps, edge devices, real-time processing |

---

## ğŸ¨ Key Features

### ğŸ–¼ï¸ Single Image Prediction
- Upload any facial image
- Get instant emotion prediction
- View confidence scores for all classes
- Visualize prediction confidence

### ğŸ“ Batch Prediction
- Upload multiple images at once
- Process entire folders
- Export results to CSV
- Statistical summary of predictions
- Batch visualization

### ğŸ“Š Model Comparison
- Side-by-side performance metrics
- Interactive visualization
- Real-time model switching
- Confidence score comparison

### ğŸ“ˆ Advanced Analytics
- Confusion matrices
- Per-class performance metrics
- Confidence distribution analysis
- Error pattern visualization

---

## ğŸš€ Installation

### Prerequisites
- Python 3.9 or higher
- pip or PDM package manager
- 4GB+ RAM recommended
- GPU optional (but recommended for training)

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/emotion-recognition.git
cd emotion-recognition
```

### Step 2: Install Dependencies

**Using PDM (Recommended):**
```bash
# Install PDM if not already installed
pip install pdm

# Install project dependencies
pdm install
```

**Using pip:**
```bash
pip install -r requirements.txt
```

### Step 3: Download Models
Models are included in the repository under `/model` directory:
- `/model/cnn/` - Custom CNN model
- `/model/resnet/` - ResNet50 model
- `/model/mobilenet/` - MobileNetV2 model
- `/model/vgg/` - VGG16 model

---

## ğŸ’» Usage

### Running Locally

#### Using PDM:
```bash
pdm run streamlit run src/app.py
```

#### Using Python directly:
```bash
streamlit run src/app.py
```

The application will open in your browser at `http://localhost:8501`

### Using the Dashboard

#### 1. Select Model
- Choose from 4 available models in the sidebar
- View model specifications and performance metrics

#### 2. Single Image Prediction
```python
# Select "Single Image" mode
# Upload an image (JPG, PNG, JPEG)
# Click "Predict Emotion"
# View results with confidence scores
```

#### 3. Batch Prediction
```python
# Select "Batch Images" mode
# Upload multiple images
# Process all images at once
# Download results as CSV
```

### Example Code Usage
```python
from tensorflow.keras.models import load_model
from utils.preprocessor import ImagePreprocessor
from PIL import Image

# Load model
model = load_model('model/vgg/vgg16_model.keras')

# Initialize preprocessor
preprocessor = ImagePreprocessor()

# Load and preprocess image
image = Image.open('path/to/image.jpg')
processed_image = preprocessor.preprocess_image(image, 'VGG16')

# Make prediction
prediction = model.predict(processed_image)
emotion = ['Angry', 'Sad', 'Surprise'][prediction.argmax()]
confidence = prediction.max()

print(f"Emotion: {emotion}")
print(f"Confidence: {confidence:.2%}")
```

---

## ğŸŒ Live Demo

### ğŸ‰ Try it Now!
Access the live demo at: **[Emotion Recognition Dashboard](https://modul6ml-uaphaidar-dimas-heryanto.streamlit.app/)**

### Demo Features:
- âœ… All 4 models available
- âœ… Single & batch image prediction
- âœ… Real-time results
- âœ… Interactive visualizations
- âœ… No installation required

### Sample Images:
You can test the system with the validation dataset or your own images!

---

## ğŸ“ Project Structure
```
emotion-recognition/
â”‚
â”œâ”€â”€ model/                          # Trained models
â”‚   â”œâ”€â”€ cnn/                       # Custom CNN
â”‚   â”‚   â”œâ”€â”€ cnn_model.keras
â”‚   â”‚   â”œâ”€â”€ cnn_model_config.json
â”‚   â”‚   â”œâ”€â”€ cnn_model_metrics.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ resnet/                    # ResNet50
â”‚   â”‚   â”œâ”€â”€ resnet50_model.keras
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ mobilenet/                 # MobileNetV2
â”‚   â”‚   â”œâ”€â”€ mobilenetv2_model.keras
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ vgg/                       # VGG16
â”‚       â”œâ”€â”€ vgg16_model.keras
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ app.py                     # Main Streamlit app
â”‚   â”œâ”€â”€ utils/                     # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py        # Model loading logic
â”‚   â”‚   â””â”€â”€ preprocessor.py        # Image preprocessing
â”‚   â””â”€â”€ pages/                     # UI pages
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ home.py
â”‚       â”œâ”€â”€ single_prediction.py
â”‚       â””â”€â”€ batch_prediction.py
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â”œâ”€â”€ training_cnn.ipynb
â”‚   â”œâ”€â”€ training_resnet50.ipynb
â”‚   â”œâ”€â”€ training_mobilenetv2.ipynb
â”‚   â””â”€â”€ training_vgg16.ipynb
â”‚
â”œâ”€â”€ assets/                        # Static assets
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ pyproject.toml                 # PDM configuration
â”œâ”€â”€ requirements.txt               # Pip requirements
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Technologies Used

### Deep Learning Frameworks
- ![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15+-orange?logo=tensorflow)
- ![Keras](https://img.shields.io/badge/Keras-API-red?logo=keras)

### Web Framework
- ![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red?logo=streamlit)

### Data Processing
- ![NumPy](https://img.shields.io/badge/NumPy-1.24+-blue?logo=numpy)
- ![Pandas](https://img.shields.io/badge/Pandas-2.0+-darkblue?logo=pandas)
- ![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green?logo=opencv)

### Visualization
- ![Plotly](https://img.shields.io/badge/Plotly-5.17+-purple?logo=plotly)
- ![Matplotlib](https://img.shields.io/badge/Matplotlib-3.7+-blue)
- ![Seaborn](https://img.shields.io/badge/Seaborn-0.12+-lightblue)

### Package Management
- ![PDM](https://img.shields.io/badge/PDM-Package_Manager-blueviolet)

---

## ğŸ“ˆ Future Improvements

### ğŸ”® Planned Features
1. **Additional Emotions**: Expand to 7 emotions (Happy, Fear, Disgust, Neutral)
2. **Real-time Video**: Webcam integration for live emotion detection
3. **Model Ensemble**: Combine predictions from multiple models
4. **API Development**: RESTful API for integration
5. **Mobile App**: Flutter/React Native implementation
6. **Edge Deployment**: TensorFlow Lite conversion

### ğŸ”¬ Research Directions
- Attention mechanisms for better feature extraction
- Self-supervised learning approaches
- Cross-dataset generalization
- Adversarial training for robustness

---

## ğŸ“š References

### Dataset
- Kapadnis, S. (2023). *Emotion Recognition Dataset*. Kaggle. [Link](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset)

### Model Architectures
- He, K., et al. (2016). *Deep Residual Learning for Image Recognition*. CVPR.
- Sandler, M., et al. (2018). *MobileNetV2: Inverted Residuals and Linear Bottlenecks*. CVPR.
- Simonyan, K., & Zisserman, A. (2014). *Very Deep Convolutional Networks*. ICLR.

### Tools & Frameworks
- Abadi, M., et al. (2016). *TensorFlow: Large-Scale Machine Learning*. OSDI.
- Streamlit Inc. (2023). *Streamlit Documentation*. [Link](https://docs.streamlit.io/)

---

## ğŸ‘¥ Contributors

### Project Team
- **[Haidar Dimas Heryanto]** - Lead Developer & ML Engineer
  - Model training and optimization
  - Web application development
  - Documentation

### Academic Supervisor
- **[Supervisor Name]** - Course Instructor
  - Project guidance
  - Technical consultation

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Kaggle** for providing the emotion recognition dataset
- **TensorFlow** and **Keras** teams for the excellent deep learning framework
- **Streamlit** for the intuitive web framework
- **Open Source Community** for pre-trained models and resources

---

## ğŸ“ Contact

### Project Links
- **Live Demo**: [Streamlit Cloud](https://modul6ml-uaphaidar-dimas-heryanto.streamlit.app/)
- **Repository**: [GitHub](https://github.com/Dimashery/Modul_6_ML-UAP_Haidar-Dimas-Heryanto)
- **Dataset**: [Kaggle](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset)

### Get in Touch
- ğŸ“§ Email: haidardimas003@gmail.com
- ğŸ’¼ LinkedIn: -
- ğŸ± GitHub: [@Dimashery](https://github.com/Dimashery)

---

<div align="center">
  
### â­ Star this repository if you found it helpful!

**Made with â¤ï¸ for Machine Learning Course - Semester 7**

</div>

---

## ğŸ“Š Quick Stats

<div align="center">

| Metric | Value |
|--------|-------|
| ğŸ¯ Best Accuracy | 87.0% (VGG16) |
| ğŸ“¦ Models Trained | 4 |
| ğŸ­ Emotions Detected | 3 |
| ğŸ“¸ Test Images | 1,294 |
| âš¡ Inference Time | <100ms |
| ğŸ† Best F1-Score | 0.84 (VGG16) |

</div>

---

**Last Updated**: December 2024 | **Version**: 1.0.0
